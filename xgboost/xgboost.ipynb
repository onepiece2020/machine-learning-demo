{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. XGBoost类库概述\n",
    "　　　　XGBoost除了支持Python外，也支持R，Java等语言。本文关注于Python的XGBoost类库，安装使用\"pip install xgboost\"即可，目前使用的是XGBoost的0.90版本。XGBoost类库除了支持决策树作为弱学习器外，还支持线性分类器，以及带DropOut的决策树DART，不过通常情况下，我们使用默认的决策树弱学习器即可，本文也只会讨论使用默认决策树弱学习器的XGBoost。\n",
    "\n",
    "　　　　XGBoost有2种Python接口风格。一种是XGBoost自带的原生Python API接口，另一种是sklearn风格的API接口，两者的实现是基本一样的，仅仅有细微的API使用的不同，主要体现在参数命名上，以及数据集的初始化上面。\n",
    "\n",
    "# 2. XGBoost类库的基本使用方式\n",
    "　　　　完整示例参见我的Github代码。\n",
    "\n",
    "# 2.1 使用原生Python API接口\n",
    "　　　　XGBoost的类库的2种接口风格，我们先来看看原生Python API接口如何使用。\n",
    "\n",
    "　　　　原生XGBoost需要先把数据集按输入特征部分，输出部分分开，然后放到一个DMatrix数据结构里面，这个DMatrix我们不需要关心里面的细节，使用我们的训练集X和y初始化即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "import matplotlib.pylab as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package xgboost:\n",
      "\n",
      "NAME\n",
      "    xgboost - XGBoost: eXtreme Gradient Boosting library.\n",
      "\n",
      "DESCRIPTION\n",
      "    Contributors: https://github.com/dmlc/xgboost/blob/master/CONTRIBUTORS.md\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    callback\n",
      "    compat\n",
      "    core\n",
      "    libpath\n",
      "    plotting\n",
      "    rabit\n",
      "    sklearn\n",
      "    training\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        xgboost.core.Booster\n",
      "        xgboost.core.DMatrix\n",
      "    sklearn.base.BaseEstimator(builtins.object)\n",
      "        xgboost.sklearn.XGBModel\n",
      "            xgboost.sklearn.XGBClassifier(xgboost.sklearn.XGBModel, sklearn.base.ClassifierMixin)\n",
      "                xgboost.sklearn.XGBRFClassifier\n",
      "            xgboost.sklearn.XGBRanker\n",
      "            xgboost.sklearn.XGBRegressor(xgboost.sklearn.XGBModel, sklearn.base.RegressorMixin)\n",
      "                xgboost.sklearn.XGBRFRegressor\n",
      "    \n",
      "    class Booster(builtins.object)\n",
      "     |  Booster(params=None, cache=(), model_file=None)\n",
      "     |  \n",
      "     |  A Booster of XGBoost.\n",
      "     |  \n",
      "     |  Booster is the model of xgboost, that contains low level routines for\n",
      "     |  training, prediction and evaluation.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __copy__(self)\n",
      "     |  \n",
      "     |  __deepcopy__(self, _)\n",
      "     |  \n",
      "     |  __del__(self)\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __init__(self, params=None, cache=(), model_file=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      params : dict\n",
      "     |          Parameters for boosters.\n",
      "     |      cache : list\n",
      "     |          List of cache items.\n",
      "     |      model_file : string\n",
      "     |          Path to the model file.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  attr(self, key)\n",
      "     |      Get attribute string from the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      key : str\n",
      "     |          The key to get attribute from.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      value : str\n",
      "     |          The attribute value of the key, returns None if attribute do not exist.\n",
      "     |  \n",
      "     |  attributes(self)\n",
      "     |      Get attributes stored in the Booster as a dictionary.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result : dictionary of  attribute_name: attribute_value pairs of strings.\n",
      "     |          Returns an empty dict if there's no attributes.\n",
      "     |  \n",
      "     |  boost(self, dtrain, grad, hess)\n",
      "     |      Boost the booster for one iteration, with customized gradient\n",
      "     |      statistics.  Like :func:`xgboost.core.Booster.update`, this\n",
      "     |      function should not be called directly by users.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dtrain : DMatrix\n",
      "     |          The training DMatrix.\n",
      "     |      grad : list\n",
      "     |          The first order of gradient.\n",
      "     |      hess : list\n",
      "     |          The second order of gradient.\n",
      "     |  \n",
      "     |  copy(self)\n",
      "     |      Copy the booster object.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster: `Booster`\n",
      "     |          a copied booster model\n",
      "     |  \n",
      "     |  dump_model(self, fout, fmap='', with_stats=False, dump_format='text')\n",
      "     |      Dump model into a text or JSON file.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fout : string\n",
      "     |          Output file name.\n",
      "     |      fmap : string, optional\n",
      "     |          Name of the file containing feature map names.\n",
      "     |      with_stats : bool, optional\n",
      "     |          Controls whether the split statistics are output.\n",
      "     |      dump_format : string, optional\n",
      "     |          Format of model dump file. Can be 'text' or 'json'.\n",
      "     |  \n",
      "     |  eval(self, data, name='eval', iteration=0)\n",
      "     |      Evaluate the model on mat.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      \n",
      "     |      name : str, optional\n",
      "     |          The name of the dataset.\n",
      "     |      \n",
      "     |      iteration : int, optional\n",
      "     |          The current iteration number.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result: str\n",
      "     |          Evaluation result string.\n",
      "     |  \n",
      "     |  eval_set(self, evals, iteration=0, feval=None)\n",
      "     |      Evaluate a set of data.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      evals : list of tuples (DMatrix, string)\n",
      "     |          List of items to be evaluated.\n",
      "     |      iteration : int\n",
      "     |          Current iteration.\n",
      "     |      feval : function\n",
      "     |          Custom evaluation function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      result: str\n",
      "     |          Evaluation result string.\n",
      "     |  \n",
      "     |  get_dump(self, fmap='', with_stats=False, dump_format='text')\n",
      "     |      Returns the model dump as a list of strings.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fmap : string, optional\n",
      "     |          Name of the file containing feature map names.\n",
      "     |      with_stats : bool, optional\n",
      "     |          Controls whether the split statistics are output.\n",
      "     |      dump_format : string, optional\n",
      "     |          Format of model dump. Can be 'text' or 'json'.\n",
      "     |  \n",
      "     |  get_fscore(self, fmap='')\n",
      "     |      Get feature importance of each feature.\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      .. note:: Zero-importance features will not be included\n",
      "     |      \n",
      "     |         Keep in mind that this function does not include zero-importance feature, i.e.\n",
      "     |         those features that have not been used in any split conditions.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fmap: str (optional)\n",
      "     |         The name of feature map file\n",
      "     |  \n",
      "     |  get_score(self, fmap='', importance_type='weight')\n",
      "     |      Get feature importance of each feature.\n",
      "     |      Importance type can be defined as:\n",
      "     |      \n",
      "     |      * 'weight': the number of times a feature is used to split the data across all trees.\n",
      "     |      * 'gain': the average gain across all splits the feature is used in.\n",
      "     |      * 'cover': the average coverage across all splits the feature is used in.\n",
      "     |      * 'total_gain': the total gain across all splits the feature is used in.\n",
      "     |      * 'total_cover': the total coverage across all splits the feature is used in.\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fmap: str (optional)\n",
      "     |         The name of feature map file.\n",
      "     |      importance_type: str, default 'weight'\n",
      "     |          One of the importance types defined above.\n",
      "     |  \n",
      "     |  get_split_value_histogram(self, feature, fmap='', bins=None, as_pandas=True)\n",
      "     |      Get split value histogram of a feature\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      feature: str\n",
      "     |          The name of the feature.\n",
      "     |      fmap: str (optional)\n",
      "     |          The name of feature map file.\n",
      "     |      bin: int, default None\n",
      "     |          The maximum number of bins.\n",
      "     |          Number of bins equals number of unique split values n_unique,\n",
      "     |          if bins == None or bins > n_unique.\n",
      "     |      as_pandas: bool, default True\n",
      "     |          Return pd.DataFrame when pandas is installed.\n",
      "     |          If False or pandas is not installed, return numpy ndarray.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a histogram of used splitting values for the specified feature\n",
      "     |      either as numpy array or pandas DataFrame.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature_names) will not be loaded.\n",
      "     |      To preserve all attributes, pickle the Booster object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  load_rabit_checkpoint(self)\n",
      "     |      Initialize the model by load from rabit checkpoint.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      version: integer\n",
      "     |          The version number of the model.\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=0, pred_leaf=False, pred_contribs=False, approx_contribs=False, pred_interactions=False, validate_features=True)\n",
      "     |      Predict with data.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``bst.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      \n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      pred_leaf : bool\n",
      "     |          When this option is on, the output will be a matrix of (nsample, ntrees)\n",
      "     |          with each record indicating the predicted leaf index of each sample in each tree.\n",
      "     |          Note that the leaf index of a tree is unique per tree, so you may find leaf 1\n",
      "     |          in both tree 1 and tree 0.\n",
      "     |      \n",
      "     |      pred_contribs : bool\n",
      "     |          When this is True the output will be a matrix of size (nsample, nfeats + 1)\n",
      "     |          with each record indicating the feature contributions (SHAP values) for that\n",
      "     |          prediction. The sum of all feature contributions is equal to the raw untransformed\n",
      "     |          margin value of the prediction. Note the final column is the bias term.\n",
      "     |      \n",
      "     |      approx_contribs : bool\n",
      "     |          Approximate the contributions of each feature\n",
      "     |      \n",
      "     |      pred_interactions : bool\n",
      "     |          When this is True the output will be a matrix of size (nsample, nfeats + 1, nfeats + 1)\n",
      "     |          indicating the SHAP interaction values for each pair of features. The sum of each\n",
      "     |          row (or column) of the interaction values equals the corresponding SHAP value (from\n",
      "     |          pred_contribs), and the sum of the entire matrix equals the raw untransformed margin\n",
      "     |          value of the prediction. Note the last row and column correspond to the bias term.\n",
      "     |      \n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature_names) will not be saved.\n",
      "     |      To preserve all attributes, pickle the Booster object.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  save_rabit_checkpoint(self)\n",
      "     |      Save the current booster to rabit checkpoint.\n",
      "     |  \n",
      "     |  save_raw(self)\n",
      "     |      Save the model to a in memory buffer representation\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      a in memory buffer representation of the model\n",
      "     |  \n",
      "     |  set_attr(self, **kwargs)\n",
      "     |      Set the attribute of the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      **kwargs\n",
      "     |          The attributes to set. Setting a value to None deletes an attribute.\n",
      "     |  \n",
      "     |  set_param(self, params, value=None)\n",
      "     |      Set parameters into the Booster.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      params: dict/list/str\n",
      "     |         list of key,value pairs, dict of key to value or simply str key\n",
      "     |      value: optional\n",
      "     |         value of the specified parameter, when params is str key\n",
      "     |  \n",
      "     |  trees_to_dataframe(self, fmap='')\n",
      "     |      Parse a boosted tree model text dump into a pandas DataFrame structure.\n",
      "     |      \n",
      "     |      This feature is only defined when the decision tree model is chosen as base\n",
      "     |      learner (`booster in {gbtree, dart}`). It is not defined for other base learner\n",
      "     |      types, such as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fmap: str (optional)\n",
      "     |         The name of feature map file.\n",
      "     |  \n",
      "     |  update(self, dtrain, iteration, fobj=None)\n",
      "     |      Update for one iteration, with objective function calculated\n",
      "     |      internally.  This function should not be called directly by users.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      dtrain : DMatrix\n",
      "     |          Training data.\n",
      "     |      iteration : int\n",
      "     |          Current iteration number.\n",
      "     |      fobj : function\n",
      "     |          Customized objective function.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data and other attributes defined here:\n",
      "     |  \n",
      "     |  feature_names = None\n",
      "    \n",
      "    class DMatrix(builtins.object)\n",
      "     |  DMatrix(data, label=None, missing=None, weight=None, silent=False, feature_names=None, feature_types=None, nthread=None)\n",
      "     |  \n",
      "     |  Data Matrix used in XGBoost.\n",
      "     |  \n",
      "     |  DMatrix is a internal data structure that used by XGBoost\n",
      "     |  which is optimized for both memory efficiency and training speed.\n",
      "     |  You can construct DMatrix from numpy.arrays\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __del__(self)\n",
      "     |  \n",
      "     |  __init__(self, data, label=None, missing=None, weight=None, silent=False, feature_names=None, feature_types=None, nthread=None)\n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : string/numpy.array/scipy.sparse/pd.DataFrame/dt.Frame\n",
      "     |          Data source of DMatrix.\n",
      "     |          When data is string type, it represents the path libsvm format txt file,\n",
      "     |          or binary file that xgboost can read from.\n",
      "     |      label : list or numpy 1-D array, optional\n",
      "     |          Label of the training data.\n",
      "     |      missing : float, optional\n",
      "     |          Value in the data which needs to be present as a missing value. If\n",
      "     |          None, defaults to np.nan.\n",
      "     |      weight : list or numpy 1-D array , optional\n",
      "     |          Weight for each instance.\n",
      "     |      \n",
      "     |          .. note:: For ranking task, weights are per-group.\n",
      "     |      \n",
      "     |              In ranking task, one weight is assigned to each group (not each data\n",
      "     |              point). This is because we only care about the relative ordering of\n",
      "     |              data points within each group, so it doesn't make sense to assign\n",
      "     |              weights to individual data points.\n",
      "     |      \n",
      "     |      silent : boolean, optional\n",
      "     |          Whether print messages during construction\n",
      "     |      feature_names : list, optional\n",
      "     |          Set names for features.\n",
      "     |      feature_types : list, optional\n",
      "     |          Set types for features.\n",
      "     |      nthread : integer, optional\n",
      "     |          Number of threads to use for loading data from numpy array. If -1,\n",
      "     |          uses maximum threads available on the system.\n",
      "     |  \n",
      "     |  get_base_margin(self)\n",
      "     |      Get the base margin of the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      base_margin : float\n",
      "     |  \n",
      "     |  get_float_info(self, field)\n",
      "     |      Get float property from the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      info : array\n",
      "     |          a numpy array of float information of the data\n",
      "     |  \n",
      "     |  get_label(self)\n",
      "     |      Get the label of the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      label : array\n",
      "     |  \n",
      "     |  get_uint_info(self, field)\n",
      "     |      Get unsigned integer property from the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      info : array\n",
      "     |          a numpy array of unsigned integer information of the data\n",
      "     |  \n",
      "     |  get_weight(self)\n",
      "     |      Get the weight of the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      weight : array\n",
      "     |  \n",
      "     |  num_col(self)\n",
      "     |      Get the number of columns (features) in the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      number of columns : int\n",
      "     |  \n",
      "     |  num_row(self)\n",
      "     |      Get the number of rows in the DMatrix.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      number of rows : int\n",
      "     |  \n",
      "     |  save_binary(self, fname, silent=True)\n",
      "     |      Save DMatrix to an XGBoost buffer.  Saved binary can be later loaded\n",
      "     |      by providing the path to :py:func:`xgboost.DMatrix` as input.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Name of the output buffer file.\n",
      "     |      silent : bool (optional; default: True)\n",
      "     |          If set, the output is suppressed.\n",
      "     |  \n",
      "     |  set_base_margin(self, margin)\n",
      "     |      Set base margin of booster to start from.\n",
      "     |      \n",
      "     |      This can be used to specify a prediction value of\n",
      "     |      existing model to be base_margin\n",
      "     |      However, remember margin is needed, instead of transformed prediction\n",
      "     |      e.g. for logistic regression: need to put in value before logistic transformation\n",
      "     |      see also example/demo.py\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      margin: array like\n",
      "     |          Prediction margin of each datapoint\n",
      "     |  \n",
      "     |  set_float_info(self, field, data)\n",
      "     |      Set float type property into the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      data: numpy array\n",
      "     |          The array of data to be set\n",
      "     |  \n",
      "     |  set_float_info_npy2d(self, field, data)\n",
      "     |      Set float type property into the DMatrix\n",
      "     |         for numpy 2d array input\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      data: numpy array\n",
      "     |          The array of data to be set\n",
      "     |  \n",
      "     |  set_group(self, group)\n",
      "     |      Set group size of DMatrix (used for ranking).\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      group : array like\n",
      "     |          Group size of each group\n",
      "     |  \n",
      "     |  set_label(self, label)\n",
      "     |      Set label of dmatrix\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      label: array like\n",
      "     |          The label information to be set into DMatrix\n",
      "     |  \n",
      "     |  set_label_npy2d(self, label)\n",
      "     |      Set label of dmatrix\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      label: array like\n",
      "     |          The label information to be set into DMatrix\n",
      "     |          from numpy 2D array\n",
      "     |  \n",
      "     |  set_uint_info(self, field, data)\n",
      "     |      Set uint type property into the DMatrix.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      field: str\n",
      "     |          The field name of the information\n",
      "     |      \n",
      "     |      data: numpy array\n",
      "     |          The array of data to be set\n",
      "     |  \n",
      "     |  set_weight(self, weight)\n",
      "     |      Set weight of each instance.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      weight : array like\n",
      "     |          Weight for each data point\n",
      "     |      \n",
      "     |          .. note:: For ranking task, weights are per-group.\n",
      "     |      \n",
      "     |              In ranking task, one weight is assigned to each group (not each data\n",
      "     |              point). This is because we only care about the relative ordering of\n",
      "     |              data points within each group, so it doesn't make sense to assign\n",
      "     |              weights to individual data points.\n",
      "     |  \n",
      "     |  set_weight_npy2d(self, weight)\n",
      "     |      Set weight of each instance\n",
      "     |          for numpy 2D array\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      weight : array like\n",
      "     |          Weight for each data point in numpy 2D array\n",
      "     |      \n",
      "     |          .. note:: For ranking task, weights are per-group.\n",
      "     |      \n",
      "     |              In ranking task, one weight is assigned to each group (not each data\n",
      "     |              point). This is because we only care about the relative ordering of\n",
      "     |              data points within each group, so it doesn't make sense to assign\n",
      "     |              weights to individual data points.\n",
      "     |  \n",
      "     |  slice(self, rindex)\n",
      "     |      Slice the DMatrix and return a new DMatrix that only contains `rindex`.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      rindex : list\n",
      "     |          List of indices to be selected.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      res : DMatrix\n",
      "     |          A new DMatrix containing only selected indices.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  feature_names\n",
      "     |      Get feature names (column labels).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_names : list or None\n",
      "     |  \n",
      "     |  feature_types\n",
      "     |      Get feature types (column types).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_types : list or None\n",
      "    \n",
      "    class XGBClassifier(XGBModel, sklearn.base.ClassifierMixin)\n",
      "     |  XGBClassifier(max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |  \n",
      "     |  Implementation of the scikit-learn API for XGBoost classification.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of trees to fit.\n",
      "     |  verbosity : int\n",
      "     |      The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting. Deprecated. Use verbosity instead.\n",
      "     |  objective : string or callable\n",
      "     |      Specify the learning task and the corresponding learning objective or\n",
      "     |      a custom objective function to be used (see note below).\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each level.\n",
      "     |  colsample_bynode : float\n",
      "     |      Subsample ratio of columns for each split.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  importance_type: string, default \"gain\"\n",
      "     |      The feature importance type for the feature_importances_ property: either \"gain\",\n",
      "     |      \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "     |  \\*\\*kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      "     |      will result in a TypeError.\n",
      "     |  \n",
      "     |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      "     |          passed via this argument will interact properly with scikit-learn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function can be provided for the ``objective``\n",
      "     |  parameter. In this case, it should have the signature\n",
      "     |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |  y_true: array_like of shape [n_samples]\n",
      "     |      The target values\n",
      "     |  y_pred: array_like of shape [n_samples]\n",
      "     |      The predicted values\n",
      "     |  \n",
      "     |  grad: array_like of shape [n_samples]\n",
      "     |      The value of the gradient for each sample point.\n",
      "     |  hess: array_like of shape [n_samples]\n",
      "     |      The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBClassifier\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='binary:logistic', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBClassifier(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None)\n",
      "     |      Fit gradient boosting classifier\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          Weight for each instance\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int, optional\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals. If there's more than one,\n",
      "     |          will use the last. If early stopping occurs, the model will have\n",
      "     |          three additional fields: bst.best_score, bst.best_iteration and\n",
      "     |          bst.best_ntree_limit (bst.best_ntree_limit is the ntree_limit parameter\n",
      "     |          default value in predict method if not any other value is specified).\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  predict_proba(self, data, ntree_limit=None, validate_features=True)\n",
      "     |      Predict the probability of each `data` example being of a given class.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe\n",
      "     |      \n",
      "     |          For each booster object, predict can only be called from one thread.\n",
      "     |          If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |          of model object and then call predict\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |          a numpy array with the probability of each data example being of a given class.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self)\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      "     |      the full range of xgboost parameters that are not defined as member variables\n",
      "     |      in sklearn grid search.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from XGBModel:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class XGBModel(sklearn.base.BaseEstimator)\n",
      "     |  XGBModel(max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, importance_type='gain', **kwargs)\n",
      "     |  \n",
      "     |  Implementation of the Scikit-Learn API for XGBoost.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of trees to fit.\n",
      "     |  verbosity : int\n",
      "     |      The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting. Deprecated. Use verbosity instead.\n",
      "     |  objective : string or callable\n",
      "     |      Specify the learning task and the corresponding learning objective or\n",
      "     |      a custom objective function to be used (see note below).\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each level.\n",
      "     |  colsample_bynode : float\n",
      "     |      Subsample ratio of columns for each split.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  importance_type: string, default \"gain\"\n",
      "     |      The feature importance type for the feature_importances_ property: either \"gain\",\n",
      "     |      \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "     |  \\*\\*kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      "     |      will result in a TypeError.\n",
      "     |  \n",
      "     |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      "     |          passed via this argument will interact properly with scikit-learn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function can be provided for the ``objective``\n",
      "     |  parameter. In this case, it should have the signature\n",
      "     |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |  y_true: array_like of shape [n_samples]\n",
      "     |      The target values\n",
      "     |  y_pred: array_like of shape [n_samples]\n",
      "     |      The predicted values\n",
      "     |  \n",
      "     |  grad: array_like of shape [n_samples]\n",
      "     |      The value of the gradient for each sample point.\n",
      "     |  hess: array_like of shape [n_samples]\n",
      "     |      The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, importance_type='gain', **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None)\n",
      "     |      Fit the gradient boosting model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          instance weights\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals.  If there's more than one,\n",
      "     |          will use the last. Returns the model from the last iteration\n",
      "     |          (not the best one). If early stopping occurs, the model will\n",
      "     |          have three additional fields: bst.best_score, bst.best_iteration\n",
      "     |          and bst.best_ntree_limit.\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self)\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      "     |      the full range of xgboost parameters that are not defined as member variables\n",
      "     |      in sklearn grid search.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class XGBRFClassifier(XGBClassifier)\n",
      "     |  XGBRFClassifier(max_depth=3, learning_rate=1, n_estimators=100, verbosity=1, silent=None, objective='binary:logistic', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=0.8, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=0.8, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |  \n",
      "     |  Experimental implementation of the scikit-learn API for XGBoost random forest classification.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of trees to fit.\n",
      "     |  verbosity : int\n",
      "     |      The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting. Deprecated. Use verbosity instead.\n",
      "     |  objective : string or callable\n",
      "     |      Specify the learning task and the corresponding learning objective or\n",
      "     |      a custom objective function to be used (see note below).\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each level.\n",
      "     |  colsample_bynode : float\n",
      "     |      Subsample ratio of columns for each split.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  importance_type: string, default \"gain\"\n",
      "     |      The feature importance type for the feature_importances_ property: either \"gain\",\n",
      "     |      \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "     |  \\*\\*kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      "     |      will result in a TypeError.\n",
      "     |  \n",
      "     |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      "     |          passed via this argument will interact properly with scikit-learn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function can be provided for the ``objective``\n",
      "     |  parameter. In this case, it should have the signature\n",
      "     |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |  y_true: array_like of shape [n_samples]\n",
      "     |      The target values\n",
      "     |  y_pred: array_like of shape [n_samples]\n",
      "     |      The predicted values\n",
      "     |  \n",
      "     |  grad: array_like of shape [n_samples]\n",
      "     |      The value of the gradient for each sample point.\n",
      "     |  hess: array_like of shape [n_samples]\n",
      "     |      The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRFClassifier\n",
      "     |      XGBClassifier\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.ClassifierMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=1, n_estimators=100, verbosity=1, silent=None, objective='binary:logistic', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=0.8, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=0.8, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self)\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBClassifier:\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBClassifier(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None)\n",
      "     |      Fit gradient boosting classifier\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          Weight for each instance\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int, optional\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals. If there's more than one,\n",
      "     |          will use the last. If early stopping occurs, the model will have\n",
      "     |          three additional fields: bst.best_score, bst.best_iteration and\n",
      "     |          bst.best_ntree_limit (bst.best_ntree_limit is the ntree_limit parameter\n",
      "     |          default value in predict method if not any other value is specified).\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  predict_proba(self, data, ntree_limit=None, validate_features=True)\n",
      "     |      Predict the probability of each `data` example being of a given class.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe\n",
      "     |      \n",
      "     |          For each booster object, predict can only be called from one thread.\n",
      "     |          If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |          of model object and then call predict\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |          a numpy array with the probability of each data example being of a given class.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      "     |      the full range of xgboost parameters that are not defined as member variables\n",
      "     |      in sklearn grid search.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from XGBModel:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.ClassifierMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the mean accuracy on the given test data and labels.\n",
      "     |      \n",
      "     |      In multi-label classification, this is the subset accuracy\n",
      "     |      which is a harsh metric since you require for each sample that\n",
      "     |      each label set be correctly predicted.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True labels for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          Mean accuracy of self.predict(X) wrt. y.\n",
      "    \n",
      "    class XGBRFRegressor(XGBRegressor)\n",
      "     |  XGBRFRegressor(max_depth=3, learning_rate=1, n_estimators=100, verbosity=1, silent=None, objective='reg:linear', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=0.8, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=0.8, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |  \n",
      "     |  Experimental implementation of the scikit-learn API for XGBoost random forest regression.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of trees to fit.\n",
      "     |  verbosity : int\n",
      "     |      The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting. Deprecated. Use verbosity instead.\n",
      "     |  objective : string or callable\n",
      "     |      Specify the learning task and the corresponding learning objective or\n",
      "     |      a custom objective function to be used (see note below).\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each level.\n",
      "     |  colsample_bynode : float\n",
      "     |      Subsample ratio of columns for each split.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  importance_type: string, default \"gain\"\n",
      "     |      The feature importance type for the feature_importances_ property: either \"gain\",\n",
      "     |      \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "     |  \\*\\*kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      "     |      will result in a TypeError.\n",
      "     |  \n",
      "     |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      "     |          passed via this argument will interact properly with scikit-learn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function can be provided for the ``objective``\n",
      "     |  parameter. In this case, it should have the signature\n",
      "     |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |  y_true: array_like of shape [n_samples]\n",
      "     |      The target values\n",
      "     |  y_pred: array_like of shape [n_samples]\n",
      "     |      The predicted values\n",
      "     |  \n",
      "     |  grad: array_like of shape [n_samples]\n",
      "     |      The value of the gradient for each sample point.\n",
      "     |  hess: array_like of shape [n_samples]\n",
      "     |      The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRFRegressor\n",
      "     |      XGBRegressor\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=1, n_estimators=100, verbosity=1, silent=None, objective='reg:linear', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=0.8, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=0.8, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self)\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None)\n",
      "     |      Fit the gradient boosting model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          instance weights\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals.  If there's more than one,\n",
      "     |          will use the last. Returns the model from the last iteration\n",
      "     |          (not the best one). If early stopping occurs, the model will\n",
      "     |          have three additional fields: bst.best_score, bst.best_iteration\n",
      "     |          and bst.best_ntree_limit.\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      "     |      the full range of xgboost parameters that are not defined as member variables\n",
      "     |      in sklearn grid search.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from XGBModel:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "    \n",
      "    class XGBRanker(XGBModel)\n",
      "     |  XGBRanker(max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='rank:pairwise', booster='gbtree', n_jobs=-1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |  \n",
      "     |  Implementation of the Scikit-Learn API for XGBoost Ranking.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of boosted trees to fit.\n",
      "     |  verbosity : int\n",
      "     |      The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting. Deprecated. Use verbosity instead.\n",
      "     |  objective : string\n",
      "     |      Specify the learning task and the corresponding learning objective.\n",
      "     |      The objective name must start with \"rank:\".\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each level.\n",
      "     |  colsample_bynode : float\n",
      "     |      Subsample ratio of columns for each split.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  \\*\\*kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict\n",
      "     |      simultaneously will result in a TypeError.\n",
      "     |  \n",
      "     |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      "     |          passed via this argument will interact properly with scikit-learn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function is currently not supported by XGBRanker.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  Group information is required for ranking tasks.\n",
      "     |  \n",
      "     |  Before fitting the model, your data need to be sorted by group. When\n",
      "     |  fitting the model, you need to provide an additional array that\n",
      "     |  contains the size of each group.\n",
      "     |  \n",
      "     |  For example, if your original data look like:\n",
      "     |  \n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   qid |   label   |   features    |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   1   |   0       |   x_1         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   1   |   1       |   x_2         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   1   |   0       |   x_3         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   2   |   0       |   x_4         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   2   |   1       |   x_5         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   2   |   1       |   x_6         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  |   2   |   1       |   x_7         |\n",
      "     |  +-------+-----------+---------------+\n",
      "     |  \n",
      "     |  then your group array should be ``[3, 4]``.\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRanker\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='rank:pairwise', booster='gbtree', n_jobs=-1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, X, y, group, sample_weight=None, eval_set=None, sample_weight_eval_set=None, eval_group=None, eval_metric=None, early_stopping_rounds=None, verbose=False, xgb_model=None, callbacks=None)\n",
      "     |      Fit the gradient boosting model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      group : array_like\n",
      "     |          group size of training data\n",
      "     |      sample_weight : array_like\n",
      "     |          group weights\n",
      "     |      \n",
      "     |          .. note:: Weights are per-group for ranking tasks\n",
      "     |      \n",
      "     |              In ranking task, one weight is assigned to each group (not each data\n",
      "     |              point). This is because we only care about the relative ordering of\n",
      "     |              data points within each group, so it doesn't make sense to assign\n",
      "     |              weights to individual data points.\n",
      "     |      \n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          group weights on the i-th validation set.\n",
      "     |      \n",
      "     |          .. note:: Weights are per-group for ranking tasks\n",
      "     |      \n",
      "     |              In ranking task, one weight is assigned to each group (not each data\n",
      "     |              point). This is because we only care about the relative ordering of\n",
      "     |              data points within each group, so it doesn't make sense to assign\n",
      "     |              weights to individual data points.\n",
      "     |      \n",
      "     |      eval_group : list of arrays, optional\n",
      "     |          A list that contains the group size corresponds to each\n",
      "     |          (X, y) pair in eval_set\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals.  If there's more than one,\n",
      "     |          will use the last. Returns the model from the last iteration\n",
      "     |          (not the best one). If early stopping occurs, the model will\n",
      "     |          have three additional fields: bst.best_score, bst.best_iteration\n",
      "     |          and bst.best_ntree_limit.\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=0, validate_features=True)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self)\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      "     |      the full range of xgboost parameters that are not defined as member variables\n",
      "     |      in sklearn grid search.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from XGBModel:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class XGBRegressor(XGBModel, sklearn.base.RegressorMixin)\n",
      "     |  XGBRegressor(max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, importance_type='gain', **kwargs)\n",
      "     |  \n",
      "     |  Implementation of the scikit-learn API for XGBoost regression.\n",
      "     |  \n",
      "     |  Parameters\n",
      "     |  ----------\n",
      "     |  max_depth : int\n",
      "     |      Maximum tree depth for base learners.\n",
      "     |  learning_rate : float\n",
      "     |      Boosting learning rate (xgb's \"eta\")\n",
      "     |  n_estimators : int\n",
      "     |      Number of trees to fit.\n",
      "     |  verbosity : int\n",
      "     |      The degree of verbosity. Valid values are 0 (silent) - 3 (debug).\n",
      "     |  silent : boolean\n",
      "     |      Whether to print messages while running boosting. Deprecated. Use verbosity instead.\n",
      "     |  objective : string or callable\n",
      "     |      Specify the learning task and the corresponding learning objective or\n",
      "     |      a custom objective function to be used (see note below).\n",
      "     |  booster: string\n",
      "     |      Specify which booster to use: gbtree, gblinear or dart.\n",
      "     |  nthread : int\n",
      "     |      Number of parallel threads used to run xgboost.  (Deprecated, please use ``n_jobs``)\n",
      "     |  n_jobs : int\n",
      "     |      Number of parallel threads used to run xgboost.  (replaces ``nthread``)\n",
      "     |  gamma : float\n",
      "     |      Minimum loss reduction required to make a further partition on a leaf node of the tree.\n",
      "     |  min_child_weight : int\n",
      "     |      Minimum sum of instance weight(hessian) needed in a child.\n",
      "     |  max_delta_step : int\n",
      "     |      Maximum delta step we allow each tree's weight estimation to be.\n",
      "     |  subsample : float\n",
      "     |      Subsample ratio of the training instance.\n",
      "     |  colsample_bytree : float\n",
      "     |      Subsample ratio of columns when constructing each tree.\n",
      "     |  colsample_bylevel : float\n",
      "     |      Subsample ratio of columns for each level.\n",
      "     |  colsample_bynode : float\n",
      "     |      Subsample ratio of columns for each split.\n",
      "     |  reg_alpha : float (xgb's alpha)\n",
      "     |      L1 regularization term on weights\n",
      "     |  reg_lambda : float (xgb's lambda)\n",
      "     |      L2 regularization term on weights\n",
      "     |  scale_pos_weight : float\n",
      "     |      Balancing of positive and negative weights.\n",
      "     |  base_score:\n",
      "     |      The initial prediction score of all instances, global bias.\n",
      "     |  seed : int\n",
      "     |      Random number seed.  (Deprecated, please use random_state)\n",
      "     |  random_state : int\n",
      "     |      Random number seed.  (replaces seed)\n",
      "     |  missing : float, optional\n",
      "     |      Value in the data which needs to be present as a missing value. If\n",
      "     |      None, defaults to np.nan.\n",
      "     |  importance_type: string, default \"gain\"\n",
      "     |      The feature importance type for the feature_importances_ property: either \"gain\",\n",
      "     |      \"weight\", \"cover\", \"total_gain\" or \"total_cover\".\n",
      "     |  \\*\\*kwargs : dict, optional\n",
      "     |      Keyword arguments for XGBoost Booster object.  Full documentation of parameters can\n",
      "     |      be found here: https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst.\n",
      "     |      Attempting to set a parameter via the constructor args and \\*\\*kwargs dict simultaneously\n",
      "     |      will result in a TypeError.\n",
      "     |  \n",
      "     |      .. note:: \\*\\*kwargs unsupported by scikit-learn\n",
      "     |  \n",
      "     |          \\*\\*kwargs is unsupported by scikit-learn.  We do not guarantee that parameters\n",
      "     |          passed via this argument will interact properly with scikit-learn.\n",
      "     |  \n",
      "     |  Note\n",
      "     |  ----\n",
      "     |  A custom objective function can be provided for the ``objective``\n",
      "     |  parameter. In this case, it should have the signature\n",
      "     |  ``objective(y_true, y_pred) -> grad, hess``:\n",
      "     |  \n",
      "     |  y_true: array_like of shape [n_samples]\n",
      "     |      The target values\n",
      "     |  y_pred: array_like of shape [n_samples]\n",
      "     |      The predicted values\n",
      "     |  \n",
      "     |  grad: array_like of shape [n_samples]\n",
      "     |      The value of the gradient for each sample point.\n",
      "     |  hess: array_like of shape [n_samples]\n",
      "     |      The value of the second derivative for each sample point\n",
      "     |  \n",
      "     |  Method resolution order:\n",
      "     |      XGBRegressor\n",
      "     |      XGBModel\n",
      "     |      sklearn.base.BaseEstimator\n",
      "     |      sklearn.base.RegressorMixin\n",
      "     |      builtins.object\n",
      "     |  \n",
      "     |  Methods inherited from XGBModel:\n",
      "     |  \n",
      "     |  __init__(self, max_depth=3, learning_rate=0.1, n_estimators=100, verbosity=1, silent=None, objective='reg:linear', booster='gbtree', n_jobs=1, nthread=None, gamma=0, min_child_weight=1, max_delta_step=0, subsample=1, colsample_bytree=1, colsample_bylevel=1, colsample_bynode=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1, base_score=0.5, random_state=0, seed=None, missing=None, importance_type='gain', **kwargs)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  __setstate__(self, state)\n",
      "     |  \n",
      "     |  apply(self, X, ntree_limit=0)\n",
      "     |      Return the predicted leaf every tree for each sample.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like, shape=[n_samples, n_features]\n",
      "     |          Input features matrix.\n",
      "     |      \n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to 0 (use all trees).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      X_leaves : array_like, shape=[n_samples, n_trees]\n",
      "     |          For each datapoint x in X and for each tree, return the index of the\n",
      "     |          leaf x ends up in. Leaves are numbered within\n",
      "     |          ``[0; 2**(self.max_depth+1))``, possibly with gaps in the numbering.\n",
      "     |  \n",
      "     |  evals_result(self)\n",
      "     |      Return the evaluation results.\n",
      "     |      \n",
      "     |      If **eval_set** is passed to the `fit` function, you can call\n",
      "     |      ``evals_result()`` to get evaluation results for all passed **eval_sets**.\n",
      "     |      When **eval_metric** is also passed to the `fit` function, the\n",
      "     |      **evals_result** will contain the **eval_metrics** passed to the `fit` function.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      evals_result : dictionary\n",
      "     |      \n",
      "     |      Example\n",
      "     |      -------\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          param_dist = {'objective':'binary:logistic', 'n_estimators':2}\n",
      "     |      \n",
      "     |          clf = xgb.XGBModel(**param_dist)\n",
      "     |      \n",
      "     |          clf.fit(X_train, y_train,\n",
      "     |                  eval_set=[(X_train, y_train), (X_test, y_test)],\n",
      "     |                  eval_metric='logloss',\n",
      "     |                  verbose=True)\n",
      "     |      \n",
      "     |          evals_result = clf.evals_result()\n",
      "     |      \n",
      "     |      The variable **evals_result** will contain:\n",
      "     |      \n",
      "     |      .. code-block:: python\n",
      "     |      \n",
      "     |          {'validation_0': {'logloss': ['0.604835', '0.531479']},\n",
      "     |          'validation_1': {'logloss': ['0.41965', '0.17686']}}\n",
      "     |  \n",
      "     |  fit(self, X, y, sample_weight=None, eval_set=None, eval_metric=None, early_stopping_rounds=None, verbose=True, xgb_model=None, sample_weight_eval_set=None, callbacks=None)\n",
      "     |      Fit the gradient boosting model\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array_like\n",
      "     |          Feature matrix\n",
      "     |      y : array_like\n",
      "     |          Labels\n",
      "     |      sample_weight : array_like\n",
      "     |          instance weights\n",
      "     |      eval_set : list, optional\n",
      "     |          A list of (X, y) tuple pairs to use as a validation set for\n",
      "     |          early-stopping\n",
      "     |      sample_weight_eval_set : list, optional\n",
      "     |          A list of the form [L_1, L_2, ..., L_n], where each L_i is a list of\n",
      "     |          instance weights on the i-th validation set.\n",
      "     |      eval_metric : str, callable, optional\n",
      "     |          If a str, should be a built-in evaluation metric to use. See\n",
      "     |          doc/parameter.rst. If callable, a custom evaluation metric. The call\n",
      "     |          signature is func(y_predicted, y_true) where y_true will be a\n",
      "     |          DMatrix object such that you may need to call the get_label\n",
      "     |          method. It must return a str, value pair where the str is a name\n",
      "     |          for the evaluation and value is the value of the evaluation\n",
      "     |          function. This objective is always minimized.\n",
      "     |      early_stopping_rounds : int\n",
      "     |          Activates early stopping. Validation error needs to decrease at\n",
      "     |          least every <early_stopping_rounds> round(s) to continue training.\n",
      "     |          Requires at least one item in evals.  If there's more than one,\n",
      "     |          will use the last. Returns the model from the last iteration\n",
      "     |          (not the best one). If early stopping occurs, the model will\n",
      "     |          have three additional fields: bst.best_score, bst.best_iteration\n",
      "     |          and bst.best_ntree_limit.\n",
      "     |          (Use bst.best_ntree_limit to get the correct value if num_parallel_tree\n",
      "     |          and/or num_class appears in the parameters)\n",
      "     |      verbose : bool\n",
      "     |          If `verbose` and an evaluation set is used, writes the evaluation\n",
      "     |          metric measured on the validation set to stderr.\n",
      "     |      xgb_model : str\n",
      "     |          file name of stored xgb model or 'Booster' instance Xgb model to be\n",
      "     |          loaded before training (allows training continuation).\n",
      "     |      callbacks : list of callback functions\n",
      "     |          List of callback functions that are applied at end of each iteration.\n",
      "     |          It is possible to use predefined callbacks by using :ref:`callback_api`.\n",
      "     |          Example:\n",
      "     |      \n",
      "     |          .. code-block:: python\n",
      "     |      \n",
      "     |              [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "     |  \n",
      "     |  get_booster(self)\n",
      "     |      Get the underlying xgboost Booster of this model.\n",
      "     |      \n",
      "     |      This will raise an exception when fit was not called\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      booster : a xgboost booster of underlying model\n",
      "     |  \n",
      "     |  get_num_boosting_rounds(self)\n",
      "     |      Gets the number of xgboost boosting rounds.\n",
      "     |  \n",
      "     |  get_params(self, deep=False)\n",
      "     |      Get parameters.\n",
      "     |  \n",
      "     |  get_xgb_params(self)\n",
      "     |      Get xgboost type parameters.\n",
      "     |  \n",
      "     |  load_model(self, fname)\n",
      "     |      Load the model from a file.\n",
      "     |      \n",
      "     |      The model is loaded from an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string or a memory buffer\n",
      "     |          Input file name or memory buffer(see also save_raw)\n",
      "     |  \n",
      "     |  predict(self, data, output_margin=False, ntree_limit=None, validate_features=True)\n",
      "     |      Predict with `data`.\n",
      "     |      \n",
      "     |      .. note:: This function is not thread safe.\n",
      "     |      \n",
      "     |        For each booster object, predict can only be called from one thread.\n",
      "     |        If you want to run prediction using multiple thread, call ``xgb.copy()`` to make copies\n",
      "     |        of model object and then call ``predict()``.\n",
      "     |      \n",
      "     |      .. note:: Using ``predict()`` with DART booster\n",
      "     |      \n",
      "     |        If the booster object is DART type, ``predict()`` will perform dropouts, i.e. only\n",
      "     |        some of the trees will be evaluated. This will produce incorrect results if ``data`` is\n",
      "     |        not the training data. To obtain correct results on test sets, set ``ntree_limit`` to\n",
      "     |        a nonzero value, e.g.\n",
      "     |      \n",
      "     |        .. code-block:: python\n",
      "     |      \n",
      "     |          preds = bst.predict(dtest, ntree_limit=num_round)\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      data : DMatrix\n",
      "     |          The dmatrix storing the input.\n",
      "     |      output_margin : bool\n",
      "     |          Whether to output the raw untransformed margin value.\n",
      "     |      ntree_limit : int\n",
      "     |          Limit number of trees in the prediction; defaults to best_ntree_limit if defined\n",
      "     |          (i.e. it has been trained with early stopping), otherwise 0 (use all trees).\n",
      "     |      validate_features : bool\n",
      "     |          When this is True, validate that the Booster's and data's feature_names are identical.\n",
      "     |          Otherwise, it is assumed that the feature_names are the same.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      prediction : numpy array\n",
      "     |  \n",
      "     |  save_model(self, fname)\n",
      "     |      Save the model to a file.\n",
      "     |      \n",
      "     |      The model is saved in an XGBoost internal binary format which is\n",
      "     |      universal among the various XGBoost interfaces. Auxiliary attributes of\n",
      "     |      the Python Booster object (such as feature names) will not be loaded.\n",
      "     |      Label encodings (text labels to numeric labels) will be also lost.\n",
      "     |      **If you are using only the Python interface, we recommend pickling the\n",
      "     |      model object for best results.**\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      fname : string\n",
      "     |          Output file name\n",
      "     |  \n",
      "     |  set_params(self, **params)\n",
      "     |      Set the parameters of this estimator.\n",
      "     |      Modification of the sklearn method to allow unknown kwargs. This allows using\n",
      "     |      the full range of xgboost parameters that are not defined as member variables\n",
      "     |      in sklearn grid search.\n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      self\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from XGBModel:\n",
      "     |  \n",
      "     |  coef_\n",
      "     |      Coefficients property\n",
      "     |      \n",
      "     |      .. note:: Coefficients are defined only for linear learners\n",
      "     |      \n",
      "     |          Coefficients are only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      coef_ : array of shape ``[n_features]`` or ``[n_classes, n_features]``\n",
      "     |  \n",
      "     |  feature_importances_\n",
      "     |      Feature importances property\n",
      "     |      \n",
      "     |      .. note:: Feature importance is defined only for tree boosters\n",
      "     |      \n",
      "     |          Feature importance is only defined when the decision tree model is chosen as base\n",
      "     |          learner (`booster=gbtree`). It is not defined for other base learner types, such\n",
      "     |          as linear learners (`booster=gblinear`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      feature_importances_ : array of shape ``[n_features]``\n",
      "     |  \n",
      "     |  intercept_\n",
      "     |      Intercept (bias) property\n",
      "     |      \n",
      "     |      .. note:: Intercept is defined only for linear learners\n",
      "     |      \n",
      "     |          Intercept (bias) is only defined when the linear model is chosen as base\n",
      "     |          learner (`booster=gblinear`). It is not defined for other base learner types, such\n",
      "     |          as tree learners (`booster=gbtree`).\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      intercept_ : array of shape ``(1,)`` or ``[n_classes]``\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __getstate__(self)\n",
      "     |  \n",
      "     |  __repr__(self)\n",
      "     |      Return repr(self).\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Methods inherited from sklearn.base.RegressorMixin:\n",
      "     |  \n",
      "     |  score(self, X, y, sample_weight=None)\n",
      "     |      Returns the coefficient of determination R^2 of the prediction.\n",
      "     |      \n",
      "     |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      "     |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      "     |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      "     |      The best possible score is 1.0 and it can be negative (because the\n",
      "     |      model can be arbitrarily worse). A constant model that always\n",
      "     |      predicts the expected value of y, disregarding the input features,\n",
      "     |      would get a R^2 score of 0.0.\n",
      "     |      \n",
      "     |      Parameters\n",
      "     |      ----------\n",
      "     |      X : array-like, shape = (n_samples, n_features)\n",
      "     |          Test samples.\n",
      "     |      \n",
      "     |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      "     |          True values for X.\n",
      "     |      \n",
      "     |      sample_weight : array-like, shape = [n_samples], optional\n",
      "     |          Sample weights.\n",
      "     |      \n",
      "     |      Returns\n",
      "     |      -------\n",
      "     |      score : float\n",
      "     |          R^2 of self.predict(X) wrt. y.\n",
      "\n",
      "FUNCTIONS\n",
      "    cv(params, dtrain, num_boost_round=10, nfold=3, stratified=False, folds=None, metrics=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, fpreproc=None, as_pandas=True, verbose_eval=None, show_stdv=True, seed=0, callbacks=None, shuffle=True)\n",
      "        Cross-validation with given parameters.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        params : dict\n",
      "            Booster params.\n",
      "        dtrain : DMatrix\n",
      "            Data to be trained.\n",
      "        num_boost_round : int\n",
      "            Number of boosting iterations.\n",
      "        nfold : int\n",
      "            Number of folds in CV.\n",
      "        stratified : bool\n",
      "            Perform stratified sampling.\n",
      "        folds : a KFold or StratifiedKFold instance or list of fold indices\n",
      "            Sklearn KFolds or StratifiedKFolds object.\n",
      "            Alternatively may explicitly pass sample indices for each fold.\n",
      "            For ``n`` folds, **folds** should be a length ``n`` list of tuples.\n",
      "            Each tuple is ``(in,out)`` where ``in`` is a list of indices to be used\n",
      "            as the training samples for the ``n`` th fold and ``out`` is a list of\n",
      "            indices to be used as the testing samples for the ``n`` th fold.\n",
      "        metrics : string or list of strings\n",
      "            Evaluation metrics to be watched in CV.\n",
      "        obj : function\n",
      "            Custom objective function.\n",
      "        feval : function\n",
      "            Custom evaluation function.\n",
      "        maximize : bool\n",
      "            Whether to maximize feval.\n",
      "        early_stopping_rounds: int\n",
      "            Activates early stopping. CV error needs to decrease at least\n",
      "            every <early_stopping_rounds> round(s) to continue.\n",
      "            Last entry in evaluation history is the one from best iteration.\n",
      "        fpreproc : function\n",
      "            Preprocessing function that takes (dtrain, dtest, param) and returns\n",
      "            transformed versions of those.\n",
      "        as_pandas : bool, default True\n",
      "            Return pd.DataFrame when pandas is installed.\n",
      "            If False or pandas is not installed, return np.ndarray\n",
      "        verbose_eval : bool, int, or None, default None\n",
      "            Whether to display the progress. If None, progress will be displayed\n",
      "            when np.ndarray is returned. If True, progress will be displayed at\n",
      "            boosting stage. If an integer is given, progress will be displayed\n",
      "            at every given `verbose_eval` boosting stage.\n",
      "        show_stdv : bool, default True\n",
      "            Whether to display the standard deviation in progress.\n",
      "            Results are not affected, and always contains std.\n",
      "        seed : int\n",
      "            Seed used to generate the folds (passed to numpy.random.seed).\n",
      "        callbacks : list of callback functions\n",
      "            List of callback functions that are applied at end of each iteration.\n",
      "            It is possible to use predefined callbacks by using\n",
      "            :ref:`Callback API <callback_api>`.\n",
      "            Example:\n",
      "        \n",
      "            .. code-block:: python\n",
      "        \n",
      "                [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "        shuffle : bool\n",
      "            Shuffle data before creating folds.\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        evaluation history : list(string)\n",
      "    \n",
      "    plot_importance(booster, ax=None, height=0.2, xlim=None, ylim=None, title='Feature importance', xlabel='F score', ylabel='Features', importance_type='weight', max_num_features=None, grid=True, show_values=True, **kwargs)\n",
      "        Plot importance based on fitted trees.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster, XGBModel or dict\n",
      "            Booster or XGBModel instance, or dict taken by Booster.get_fscore()\n",
      "        ax : matplotlib Axes, default None\n",
      "            Target axes instance. If None, new figure and axes will be created.\n",
      "        grid : bool, Turn the axes grids on or off.  Default is True (On).\n",
      "        importance_type : str, default \"weight\"\n",
      "            How the importance is calculated: either \"weight\", \"gain\", or \"cover\"\n",
      "        \n",
      "            * \"weight\" is the number of times a feature appears in a tree\n",
      "            * \"gain\" is the average gain of splits which use the feature\n",
      "            * \"cover\" is the average coverage of splits which use the feature\n",
      "              where coverage is defined as the number of samples affected by the split\n",
      "        max_num_features : int, default None\n",
      "            Maximum number of top features displayed on plot. If None, all features will be displayed.\n",
      "        height : float, default 0.2\n",
      "            Bar height, passed to ax.barh()\n",
      "        xlim : tuple, default None\n",
      "            Tuple passed to axes.xlim()\n",
      "        ylim : tuple, default None\n",
      "            Tuple passed to axes.ylim()\n",
      "        title : str, default \"Feature importance\"\n",
      "            Axes title. To disable, pass None.\n",
      "        xlabel : str, default \"F score\"\n",
      "            X axis title label. To disable, pass None.\n",
      "        ylabel : str, default \"Features\"\n",
      "            Y axis title label. To disable, pass None.\n",
      "        show_values : bool, default True\n",
      "            Show values on plot. To disable, pass False.\n",
      "        kwargs :\n",
      "            Other keywords passed to ax.barh()\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ax : matplotlib Axes\n",
      "    \n",
      "    plot_tree(booster, fmap='', num_trees=0, rankdir='UT', ax=None, **kwargs)\n",
      "        Plot specified tree.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster, XGBModel\n",
      "            Booster or XGBModel instance\n",
      "        fmap: str (optional)\n",
      "           The name of feature map file\n",
      "        num_trees : int, default 0\n",
      "            Specify the ordinal number of target tree\n",
      "        rankdir : str, default \"UT\"\n",
      "            Passed to graphiz via graph_attr\n",
      "        ax : matplotlib Axes, default None\n",
      "            Target axes instance. If None, new figure and axes will be created.\n",
      "        kwargs :\n",
      "            Other keywords passed to to_graphviz\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ax : matplotlib Axes\n",
      "    \n",
      "    to_graphviz(booster, fmap='', num_trees=0, rankdir='UT', yes_color='#0000FF', no_color='#FF0000', condition_node_params=None, leaf_node_params=None, **kwargs)\n",
      "        Convert specified tree to graphviz instance. IPython can automatically plot the\n",
      "        returned graphiz instance. Otherwise, you should call .render() method\n",
      "        of the returned graphiz instance.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        booster : Booster, XGBModel\n",
      "            Booster or XGBModel instance\n",
      "        fmap: str (optional)\n",
      "           The name of feature map file\n",
      "        num_trees : int, default 0\n",
      "            Specify the ordinal number of target tree\n",
      "        rankdir : str, default \"UT\"\n",
      "            Passed to graphiz via graph_attr\n",
      "        yes_color : str, default '#0000FF'\n",
      "            Edge color when meets the node condition.\n",
      "        no_color : str, default '#FF0000'\n",
      "            Edge color when doesn't meet the node condition.\n",
      "        condition_node_params : dict (optional)\n",
      "            condition node configuration,\n",
      "            {'shape':'box',\n",
      "                   'style':'filled,rounded',\n",
      "                   'fillcolor':'#78bceb'\n",
      "            }\n",
      "        leaf_node_params : dict (optional)\n",
      "            leaf node configuration\n",
      "            {'shape':'box',\n",
      "                   'style':'filled',\n",
      "                   'fillcolor':'#e48038'\n",
      "            }\n",
      "        kwargs :\n",
      "            Other keywords passed to graphviz graph_attr\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        ax : matplotlib Axes\n",
      "    \n",
      "    train(params, dtrain, num_boost_round=10, evals=(), obj=None, feval=None, maximize=False, early_stopping_rounds=None, evals_result=None, verbose_eval=True, xgb_model=None, callbacks=None, learning_rates=None)\n",
      "        Train a booster with given parameters.\n",
      "        \n",
      "        Parameters\n",
      "        ----------\n",
      "        params : dict\n",
      "            Booster params.\n",
      "        dtrain : DMatrix\n",
      "            Data to be trained.\n",
      "        num_boost_round: int\n",
      "            Number of boosting iterations.\n",
      "        evals: list of pairs (DMatrix, string)\n",
      "            List of items to be evaluated during training, this allows user to watch\n",
      "            performance on the validation set.\n",
      "        obj : function\n",
      "            Customized objective function.\n",
      "        feval : function\n",
      "            Customized evaluation function.\n",
      "        maximize : bool\n",
      "            Whether to maximize feval.\n",
      "        early_stopping_rounds: int\n",
      "            Activates early stopping. Validation error needs to decrease at least\n",
      "            every **early_stopping_rounds** round(s) to continue training.\n",
      "            Requires at least one item in **evals**.\n",
      "            If there's more than one, will use the last.\n",
      "            Returns the model from the last iteration (not the best one).\n",
      "            If early stopping occurs, the model will have three additional fields:\n",
      "            ``bst.best_score``, ``bst.best_iteration`` and ``bst.best_ntree_limit``.\n",
      "            (Use ``bst.best_ntree_limit`` to get the correct value if\n",
      "            ``num_parallel_tree`` and/or ``num_class`` appears in the parameters)\n",
      "        evals_result: dict\n",
      "            This dictionary stores the evaluation results of all the items in watchlist.\n",
      "        \n",
      "            Example: with a watchlist containing\n",
      "            ``[(dtest,'eval'), (dtrain,'train')]`` and\n",
      "            a parameter containing ``('eval_metric': 'logloss')``,\n",
      "            the **evals_result** returns\n",
      "        \n",
      "            .. code-block:: python\n",
      "        \n",
      "                {'train': {'logloss': ['0.48253', '0.35953']},\n",
      "                 'eval': {'logloss': ['0.480385', '0.357756']}}\n",
      "        \n",
      "        verbose_eval : bool or int\n",
      "            Requires at least one item in **evals**.\n",
      "            If **verbose_eval** is True then the evaluation metric on the validation set is\n",
      "            printed at each boosting stage.\n",
      "            If **verbose_eval** is an integer then the evaluation metric on the validation set\n",
      "            is printed at every given **verbose_eval** boosting stage. The last boosting stage\n",
      "            / the boosting stage found by using **early_stopping_rounds** is also printed.\n",
      "            Example: with ``verbose_eval=4`` and at least one item in **evals**, an evaluation metric\n",
      "            is printed every 4 boosting stages, instead of every boosting stage.\n",
      "        learning_rates: list or function (deprecated - use callback API instead)\n",
      "            List of learning rate for each boosting round\n",
      "            or a customized function that calculates eta in terms of\n",
      "            current number of round and the total number of boosting round (e.g. yields\n",
      "            learning rate decay)\n",
      "        xgb_model : file name of stored xgb model or 'Booster' instance\n",
      "            Xgb model to be loaded before training (allows training continuation).\n",
      "        callbacks : list of callback functions\n",
      "            List of callback functions that are applied at end of each iteration.\n",
      "            It is possible to use predefined callbacks by using\n",
      "            :ref:`Callback API <callback_api>`.\n",
      "            Example:\n",
      "        \n",
      "            .. code-block:: python\n",
      "        \n",
      "                [xgb.callback.reset_learning_rate(custom_rates)]\n",
      "        \n",
      "        Returns\n",
      "        -------\n",
      "        Booster : a trained booster model\n",
      "\n",
      "DATA\n",
      "    __all__ = ['DMatrix', 'Booster', 'train', 'cv', 'XGBModel', 'XGBClassi...\n",
      "\n",
      "VERSION\n",
      "    0.90\n",
      "\n",
      "FILE\n",
      "    d:\\anaconda3\\lib\\site-packages\\xgboost\\__init__.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(xgboost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets.samples_generator import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X为样本特征，y为样本类别输出， 共10000个样本，每个样本20个特征，输出有2个类别，没有冗余特征，每个类别一个簇\n",
    "X, y = make_classification(n_samples=10000, n_features=20, n_redundant=0,\n",
    "                             n_clusters_per_class=1, n_classes=2, flip_y=0.1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "dtrain = xgb.DMatrix(X_train,y_train)\n",
    "dtest = xgb.DMatrix(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7500, 20)\n",
      "(2500, 20)\n",
      "(7500,)\n",
      "(2500,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<xgboost.core.DMatrix at 0xeae2d50>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码中，我们随机初始化了一个二分类的数据集，然后分成了训练集和验证集。使用训练集和验证集分别初始化了一个DMatrix，有了DMatrix，就可以做训练和预测了。简单的示例代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth':5, 'eta':0.5, 'verbosity':1, 'objective':'binary:logistic'}\n",
    "raw_model = xgb.train(param, dtrain, num_boost_round=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9601333333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "pred_train_raw = raw_model.predict(dtrain)\n",
    "for i in range(len(pred_train_raw)):\n",
    "    if pred_train_raw[i] > 0.5:\n",
    "         pred_train_raw[i]=1\n",
    "    else:\n",
    "        pred_train_raw[i]=0               \n",
    "print (accuracy_score(dtrain.get_label(), pred_train_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练集的准确率我这里输出是0.96。再看看验证集的表现："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9376\n"
     ]
    }
   ],
   "source": [
    "pred_test_raw = raw_model.predict(dtest)\n",
    "for i in range(len(pred_test_raw)):\n",
    "    if pred_test_raw[i] > 0.5:\n",
    "         pred_test_raw[i]=1\n",
    "    else:\n",
    "        pred_test_raw[i]=0               \n",
    "print (accuracy_score(dtest.get_label(), pred_test_raw))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证集的准确率我这里的输出是0.9376，已经很高了。\n",
    "\n",
    " 　　　　不过对于我这样用惯sklearn风格API的，还是不太喜欢原生Python API接口，既然有sklearn的wrapper，那么就尽量使用sklearn风格的接口吧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 使用sklearn风格接口，使用原生参数\n",
    "　　　　对于sklearn风格的接口，主要有2个类可以使用，一个是分类用的XGBClassifier，另一个是回归用的XGBRegressor。在使用这2个类的使用，对于算法的参数输入也有2种方式，第一种就是仍然使用和原始API一样的参数命名集合，另一种是使用sklearn风格的参数命名。我们这里先看看如何使用和原始API一样的参数命名集合。\n",
    "\n",
    "　　　　其实就是使用XGBClassifier/XGBRegressor的**kwargs参数，把上面原生参数的params集合放进去，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.062\n",
      "Will train until validation_0-error hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-error:0.062\n",
      "[2]\tvalidation_0-error:0.0624\n",
      "[3]\tvalidation_0-error:0.0624\n",
      "[4]\tvalidation_0-error:0.0624\n",
      "[5]\tvalidation_0-error:0.062\n",
      "[6]\tvalidation_0-error:0.062\n",
      "[7]\tvalidation_0-error:0.062\n",
      "[8]\tvalidation_0-error:0.062\n",
      "[9]\tvalidation_0-error:0.0612\n",
      "[10]\tvalidation_0-error:0.0616\n",
      "[11]\tvalidation_0-error:0.0616\n",
      "[12]\tvalidation_0-error:0.0616\n",
      "[13]\tvalidation_0-error:0.0616\n",
      "[14]\tvalidation_0-error:0.0612\n",
      "[15]\tvalidation_0-error:0.0616\n",
      "[16]\tvalidation_0-error:0.0612\n",
      "[17]\tvalidation_0-error:0.06\n",
      "[18]\tvalidation_0-error:0.06\n",
      "[19]\tvalidation_0-error:0.06\n",
      "[20]\tvalidation_0-error:0.06\n",
      "[21]\tvalidation_0-error:0.06\n",
      "[22]\tvalidation_0-error:0.06\n",
      "[23]\tvalidation_0-error:0.0604\n",
      "[24]\tvalidation_0-error:0.0604\n",
      "[25]\tvalidation_0-error:0.0604\n",
      "[26]\tvalidation_0-error:0.06\n",
      "[27]\tvalidation_0-error:0.0604\n",
      "Stopping. Best iteration:\n",
      "[17]\tvalidation_0-error:0.06\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, eta=0.5, gamma=0,\n",
       "       learning_rate=0.1, max_delta_step=0, max_depth=5,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, n_jobs=1,\n",
       "       nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=None, subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_model_raw = xgb.XGBClassifier(**param)\n",
    "sklearn_model_raw.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"error\",\n",
    "        eval_set=[(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "里面的param其实就是2.1节里面定义的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = {'max_depth':5, 'eta':0.5, 'verbosity':1, 'objective':'binary:logistic'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用sklearn风格的接口，却使用原始的参数名定义，感觉还是有点怪，所以我一般还是习惯使用另一种风格接口，sklearn风格的参数命名。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 使用sklearn风格接口，使用sklearn风格参数\n",
    "　　　　使用sklearn风格的接口，并使用sklearn风格的参数，是我推荐的方式，主要是这样做和GBDT之类的sklearn库使用起来没有什么两样了，也可以使用sklearn的网格搜索。\n",
    "\n",
    "　　　　不过这样做的话，参数定义命名和2.1与2.2节就有些不同了。具体的参数意义我们后面讲，我们看看分类的算法初始化，训练与调用的简单过程："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model_new = xgb.XGBClassifier(max_depth=5,\n",
    "                                      learning_rate= 0.5, \n",
    "                                      verbosity=1, \n",
    "                                      objective='binary:logistic',\n",
    "                                      random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到，参数定义直接放在了XGBClassifier的类参数里，和sklearn类似。大家可以看到之前两节我们定义的步长eta，这里变成了另一个名字learning_rate。\n",
    "\n",
    "　　　　在初始化后，训练和预测的方法就和2.2节没有区别了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.062\n",
      "Will train until validation_0-error hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-error:0.0624\n",
      "[2]\tvalidation_0-error:0.0584\n",
      "[3]\tvalidation_0-error:0.0608\n",
      "[4]\tvalidation_0-error:0.0596\n",
      "[5]\tvalidation_0-error:0.06\n",
      "[6]\tvalidation_0-error:0.0604\n",
      "[7]\tvalidation_0-error:0.0592\n",
      "[8]\tvalidation_0-error:0.0592\n",
      "[9]\tvalidation_0-error:0.0596\n",
      "[10]\tvalidation_0-error:0.06\n",
      "[11]\tvalidation_0-error:0.0608\n",
      "[12]\tvalidation_0-error:0.0608\n",
      "Stopping. Best iteration:\n",
      "[2]\tvalidation_0-error:0.0584\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.5,\n",
       "       max_delta_step=0, max_depth=5, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=1, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_model_new.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"error\",\n",
    "        eval_set=[(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. XGBoost类库参数\n",
    "　　　　在第二节我们已经尝试使用XGBoost类库了，但是对于XGBoost的类库参数并没有过多讨论。这里我们就详细讨论下，主要以2.3节的sklearn风格参数为主来进行讨论。这些参数我会和之前讲的scikit-learn 梯度提升树(GBDT)调参小结中的参数定义对应，这样如果大家对GBDT的调参很熟悉了，那么XGBoost的调参也就掌握90%了。\n",
    "\n",
    "　　　　XGBoost的类库参数主要包括boosting框架参数，弱学习器参数以及其他参数。\n",
    "\n",
    "# 3.1  XGBoost框架参数　\n",
    "　　　　对于XGBoost的框架参数，最重要的是3个参数: booster，n_estimators和objectve。\n",
    "\n",
    "　　　　1) booster决定了XGBoost使用的弱学习器类型，可以是默认的gbtree, 也就是CART决策树，还可以是线性弱学习器gblinear以及DART。一般来说，我们使用gbtree就可以了，不需要调参。\n",
    "\n",
    "　　　　2) n_estimators则是非常重要的要调的参数，它关系到我们XGBoost模型的复杂度，因为它代表了我们决策树弱学习器的个数。这个参数对应sklearn GBDT的n_estimators。n_estimators太小，容易欠拟合，n_estimators太大，模型会过于复杂，一般需要调参选择一个适中的数值。\n",
    "\n",
    "　　　　3) objective代表了我们要解决的问题是分类还是回归，或其他问题，以及对应的损失函数。具体可以取的值很多，一般我们只关心在分类和回归的时候使用的参数。\n",
    "\n",
    "　　　　在回归问题objective一般使用reg:squarederror ，即MSE均方误差。二分类问题一般使用binary:logistic, 多分类问题一般使用multi:softmax。\n",
    "\n",
    "#  3.2  XGBoost 弱学习器参数   \n",
    "　　　　这里我们只讨论使用gbtree默认弱学习器的参数。  要调参的参数主要是决策树的相关参数如下： \n",
    "\n",
    "　　　　1)   max_depth: 控制树结构的深度，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，需要限制这个最大深度，具体的取值一般要网格搜索调参。这个参数对应sklearn GBDT的max_depth。\n",
    "\n",
    "　　　　2) min_child_weight: 最小的子节点权重阈值，如果某个树节点的权重小于这个阈值，则不会再分裂子树，即这个树节点就是叶子节点。这里树节点的权重使用的是该节点所有样本的二阶导数的和，即XGBoost原理篇里面的Htj:\n",
    "Htj=∑xi∈Rtj hti\n",
    "　　　　这个值需要网格搜索寻找最优值，在sklearn GBDT里面，没有完全对应的参数，不过min_samples_split从另一个角度起到了阈值限制。\n",
    "\n",
    "　　　　3) gamma: XGBoost的决策树分裂所带来的损失减小阈值。也就是我们在尝试树结构分裂时，会尝试最大数下式：\n",
    "max12G2LHL+λ+12G2RHR+λ−12(GL+GR)2HL+HR+λ−γ\n",
    "　　　　这个最大化后的值需要大于我们的gamma，才能继续分裂子树。这个值也需要网格搜索寻找最优值。\n",
    "\n",
    "　　　　4) subsample: 子采样参数，这个也是不放回抽样，和sklearn GBDT的subsample作用一样。选择小于1的比例可以减少方差，即防止过拟合，但是会增加样本拟合的偏差，因此取值不能太低。初期可以取值1，如果发现过拟合后可以网格搜索调参找一个相对小一些的值。\n",
    "\n",
    "　　　　5) colsample_bytree/colsample_bylevel/colsample_bynode: 这三个参数都是用于特征采样的，默认都是不做采样，即使用所有的特征建立决策树。colsample_bytree控制整棵树的特征采样比例，colsample_bylevel控制某一层的特征采样比例，而colsample_bynode控制某一个树节点的特征采样比例。比如我们一共64个特征，则假设colsample_bytree，colsample_bylevel和colsample_bynode都是0.5，则某一个树节点分裂时会随机采样8个特征来尝试分裂子树。\n",
    "\n",
    "　　　　6) reg_alpha/reg_lambda: 这2个是XGBoost的正则化参数。reg_alpha是L1正则化系数，reg_lambda是L2正则化系数，在原理篇里我们讨论了XGBoost的正则化损失项部分：\n",
    "Ω(ht)=γJ+λ2∑j=1Jw2tj\n",
    "　　　　上面这些参数都是需要调参的，不过一般先调max_depth，min_child_weight和gamma。如果发现有过拟合的情况下，再尝试调后面几个参数。\n",
    "\n",
    "# 3.3  XGBoost 其他参数\n",
    "　　　　XGBoost还有一些其他的参数需要注意，主要是learning_rate。\n",
    "\n",
    "　　　　learning_rate控制每个弱学习器的权重缩减系数，和sklearn GBDT的learning_rate类似，较小的learning_rate意味着我们需要更多的弱学习器的迭代次数。通常我们用步长和迭代最大次数一起来决定算法的拟合效果。所以这两个参数n_estimators和learning_rate要一起调参才有效果。当然也可以先固定一个learning_rate ，然后调完n_estimators，再调完其他所有参数后，最后再来调learning_rate和n_estimators。\n",
    "\n",
    "　　　　此外，n_jobs控制算法的并发线程数， scale_pos_weight用于类别不平衡的时候，负例和正例的比例。类似于sklearn中的class_weight。importance_type则可以查询各个特征的重要性程度。可以选择“gain”, “weight”, “cover”, “total_gain” 或者 “total_cover”。最后可以通过调用booster的get_score方法获取对应的特征权重。“weight”通过特征被选中作为分裂特征的计数来计算重要性，“gain”和“total_gain”则通过分别计算特征被选中做分裂特征时带来的平均增益和总增益来计算重要性。“cover”和 “total_cover”通过计算特征被选中做分裂时的平均样本覆盖度和总体样本覆盖度来来计算重要性。\n",
    "\n",
    "# 4. XGBoost网格搜索调参\n",
    "　　　　XGBoost可以和sklearn的网格搜索类GridSeachCV结合使用来调参，使用时和普通sklearn分类回归算法没有区别。具体的流程的一个示例如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.5,\n",
       "       max_delta_step=0, max_depth=5, min_child_weight=1, missing=None,\n",
       "       n_estimators=100, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=1, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [4, 5, 6], 'n_estimators': [5, 10, 20]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gsCv = GridSearchCV(sklearn_model_new,\n",
    "                   {'max_depth': [4,5,6],\n",
    "                    'n_estimators': [5,10,20]})\n",
    "gsCv.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9441333333333334\n",
      "{'max_depth': 6, 'n_estimators': 5}\n"
     ]
    }
   ],
   "source": [
    "print(gsCv.best_score_)\n",
    "print(gsCv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我这里的输出是：\n",
    "\n",
    "　　　　0.9441333333333334\n",
    "\n",
    "　　　　{'max_depth': 6, 'n_estimators': 5}\n",
    "\n",
    "　　　　接着尝试在上面搜索的基础上调learning_rate ："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\preprocessing\\label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.1,\n",
       "       max_delta_step=0, max_depth=6, min_child_weight=1, missing=None,\n",
       "       n_estimators=5, n_jobs=1, nthread=None, objective='binary:logistic',\n",
       "       random_state=1, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "       seed=None, silent=None, subsample=1, verbosity=1),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'learning_rate ': [0.3, 0.5, 0.7]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_model_new2 = xgb.XGBClassifier(max_depth=6,\n",
    "                                       n_estimators=5,\n",
    "                                       verbosity=1, \n",
    "                                       objective='binary:logistic',\n",
    "                                       random_state=1)\n",
    "gsCv2 = GridSearchCV(sklearn_model_new2, \n",
    "                   {'learning_rate ': [0.3,0.5,0.7]})\n",
    "gsCv2.fit(X_train,y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9437333333333333\n",
      "{'learning_rate ': 0.3}\n"
     ]
    }
   ],
   "source": [
    "print(gsCv2.best_score_)\n",
    "print(gsCv2.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我这里的输出是：\n",
    "\n",
    "　　　　0.9437333333333333\n",
    "\n",
    "　　　　{'learning_rate ': 0.3}\n",
    "\n",
    "　　　　当然实际情况这里需要继续调参，这里假设我们已经调参完毕，我们尝试用验证集看看效果：\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-error:0.062\n",
      "Will train until validation_0-error hasn't improved in 10 rounds.\n",
      "[1]\tvalidation_0-error:0.0616\n",
      "[2]\tvalidation_0-error:0.0616\n",
      "[3]\tvalidation_0-error:0.0588\n",
      "[4]\tvalidation_0-error:0.06\n",
      "[5]\tvalidation_0-error:0.0604\n",
      "[6]\tvalidation_0-error:0.0592\n",
      "[7]\tvalidation_0-error:0.0592\n",
      "[8]\tvalidation_0-error:0.0592\n",
      "[9]\tvalidation_0-error:0.0596\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bynode=1, colsample_bytree=1, gamma=0, learning_rate=0.3,\n",
       "       max_delta_step=0, max_depth=4, min_child_weight=1, missing=None,\n",
       "       n_estimators=10, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0,\n",
       "       reg_lambda=1, scale_pos_weight=1, seed=None, silent=None,\n",
       "       subsample=1, verbosity=1)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sklearn_model_new2 = xgb.XGBClassifier(max_depth=4,\n",
    "                                       learning_rate= 0.3, \n",
    "                                       verbosity=1, \n",
    "                                       objective='binary:logistic',\n",
    "                                       n_estimators=10)\n",
    "sklearn_model_new2.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"error\",\n",
    "        eval_set=[(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后的输出是：\n",
    "\n",
    "　　　　[9]\tvalidation_0-error:0.0596\n",
    "　　　　也就是验证集的准确率是94.04%。\n",
    "\n",
    "　　　　我们可以通过验证集的准确率来判断我们前面网格搜索调参是否起到了效果。实际处理的时候需要反复搜索参数并验证。\n",
    "\n",
    "　　　　以上就是XGBoost的类库使用总结了，希望可以帮到要用XGBoost解决实际问题的朋友们。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
